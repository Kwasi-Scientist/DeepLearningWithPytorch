{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Importing Image or video data </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Importing Image or video data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imageio imports image into numpy arrays\n",
    "img_arr = imageio.imread('./data/bobby.jpg')\n",
    "img_arr.shape #height, width, RBG chanel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert image to proper layout for Pytorch of C, H, W\n",
    "#where 2 = chanel index, 0 = height, ...\n",
    "\n",
    "img = torch.from_numpy(img_arr)\n",
    "output = img.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want a batch of 100 RGB images of 256 pixel height and width \n",
    "#Here we init the batch and specfiy that each color be represented as an 8 bit integer\n",
    "batch_size = 100 \n",
    "batch = torch.zeros(100,3,256,256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all png images and store as a tensors\n",
    "data_dir = './data/cats/'\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] =='.png']\n",
    "for i , filename in enumerate(filenames):\n",
    "    img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "    img_t= torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2,0,1)\n",
    "    img_t = img_t[:3]\n",
    "    batch[i] = img_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.float()\n",
    "batch /=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we find the mean nad stf of input to scale it so output has zero mean and\n",
    "# std of 1 accross each channel\n",
    "n_channels = batch.shape[1]\n",
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:,c])\n",
    "    batch[:, c] = (batch[:, c] - mean)/std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Volumetric Data </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Volumetric Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CT Data comes in series of images of slices of the body from head to toe\n",
    "#they only have one channel because things are in greyscale\n",
    "#We can stack the indicidual 2D slcies into a 3D tensor to build volumentric data\n",
    "#represents the 3D anatomy of subhject \n",
    "#These tehnsors have a 5 D shape N x C x D x H x W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 56/99  (56.699/99  (100.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 512, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Us volread functiono of image io to load ct scans\n",
    "dir_path = \"./data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
    "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
    "vol_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512, 99])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use Unsqueeze to make room fo the channel dimension\n",
    "vol = torch.from_numpy(vol_arr).float()\n",
    "vol = torch.transpose(vol, 0, 2)\n",
    "vol = torch.unsqueeze(vol, 0)\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create the 5D data set by stacking the volumes along batch directoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Tabular Data </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML \n",
    "<h3> Tabular Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabular data is spreadsheet data\n",
    "#We need to encode the heterogenous data into tensor floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wineData = pd.read_csv(r'./data/p1ch4/tabular-wine/winequality-white.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_numpy = wineData.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4898, 12),\n",
       " Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "        'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "        'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_numpy.shape, wineData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineTensor = torch.from_numpy(wine_numpy)\n",
    "wineTensor.shape, wineTensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We 3 types of numerical values her\n",
    "#continuous, ordinal, and categorical values\n",
    "#continuous values are like values of mass. Its ok to say that A is mass 10 and is twice as massive as B wwith mass 5\n",
    "#Ordinal values are like continuous but their fixed relationship is not true. Small = 1 medium = 2 large = 3\n",
    "#We know that large is bigger than medium but we don't know by how much\n",
    "#we cannot take an average of these values or anything\n",
    "#categorical have neither order nor numerical meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898, 11])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take out wine score coulmn ans save as yTr\n",
    "\n",
    "xTr = wineTensor[:,:-1]\n",
    "xTr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTr = wineTensor[:,-1]\n",
    "yTr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTr = yTr.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6,  ..., 6, 7, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can decide to keep yTr as is and do regression or we can use OneHotEncoder on yTr\n",
    "#if scores are purely discrete oneHotEncoder is apprpriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trye OneHotEncoder using Scatter_ method\n",
    "#Remember trailing _ methods act inplace\n",
    "yTr_onehot = torch.zeros(yTr.shape[0],10)\n",
    "yTr_onehot.scatter_(1, yTr.unsqueeze(1), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898, 10])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTr_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        ...,\n",
       "        [6],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTr_unsq= yTr.unsqueeze(1)\n",
    "yTr_unsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTr_mean = torch.mean(xTr, 0)\n",
    "xTr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
       "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find variance\n",
    "xTr_var = torch.var(xTr, 0)\n",
    "xTr_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7208e-01, -8.1762e-02,  2.1326e-01,  ..., -1.2468e+00,\n",
       "         -3.4915e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7996e-02,  ...,  7.3995e-01,\n",
       "          1.3417e-03, -8.2419e-01],\n",
       "        [ 1.4756e+00,  1.7450e-02,  5.4378e-01,  ...,  4.7505e-01,\n",
       "         -4.3677e-01, -3.3663e-01],\n",
       "        ...,\n",
       "        [-4.2043e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3130e+00,\n",
       "         -2.6153e-01, -9.0545e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0049e+00,\n",
       "         -9.6251e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7505e-01,\n",
       "         -1.4882e+00,  1.0448e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalze data \n",
    "xTr_normalized = (xTr - xTr_mean) / torch.sqrt(xTr_var)\n",
    "xTr_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), torch.bool, tensor(20))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find  rows in yTr that have a score <=3 \n",
    "bad_i = yTr <=3\n",
    "bad_i.shape, bad_i.dtype, bad_i.sum()\n",
    "#Here all values are True or false. Only 20 values are True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 11])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diplay True values\n",
    "badWines = xTr[bad_i]\n",
    "badWines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 fixed acidity          7.60   6.89   6.73\n",
      " 1 volatile acidity       0.33   0.28   0.27\n",
      " 2 citric acid            0.34   0.34   0.33\n",
      " 3 residual sugar         6.39   6.71   5.26\n",
      " 4 chlorides              0.05   0.05   0.04\n",
      " 5 free sulfur dioxide   53.33  35.42  34.55\n",
      " 6 total sulfur dioxide 170.60 141.83 125.25\n",
      " 7 density                0.99   0.99   0.99\n",
      " 8 pH                     3.19   3.18   3.22\n",
      " 9 sulphates              0.47   0.49   0.50\n",
      "10 alcohol               10.35  10.26  11.42\n"
     ]
    }
   ],
   "source": [
    "#find bad mid and good wines\n",
    "midWines = xTr[(yTr > 3) & (yTr < 7)]\n",
    "goodWines = xTr[yTr >=7]\n",
    "\n",
    "bad_mean = torch.mean(badWines, 0)\n",
    "mid_mean = torch.mean(midWines, 0)\n",
    "good_mean = torch.mean(goodWines, 0)\n",
    "\n",
    "for i, args in enumerate(zip(wineData.columns, bad_mean, mid_mean, good_mean)):\n",
    "    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), torch.bool, tensor(2727))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we find inices of wines with sulfure less than threshold\n",
    "#note Torch.lt is less than. computes x < y\n",
    "total_sulfur_threshold = 141.83\n",
    "total_sulfur_data = xTr[:,6]\n",
    "predicted_i = torch.lt(total_sulfur_data, total_sulfur_threshold)\n",
    "predicted_i.shape, predicted_i.dtype, predicted_i.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), torch.bool, tensor(3258))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_i = yTr > 5\n",
    "actual_i.shape, actual_i.dtype, actual_i.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 0.74000733406674, 0.6193984039287906)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find accuracy\n",
    "n_matches = torch.sum(actual_i & predicted_i).item()\n",
    "n_predicted = torch.sum(predicted_i).item()\n",
    "n_actual = torch.sum(actual_i).item()\n",
    "n_matches, n_matches/n_predicted, n_matches/ n_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Time Series Data </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Time Series Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will take DC Bike share data and process it\n",
    "#we will take two flad 2D data sets and make into 1 3D set\n",
    "#We want one axis to increase at one dat per index incremenat and another acis that represents hours of the da\n",
    "#third acis will be column data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_pandas = pd.read_csv(r\"./data/p1ch4/bike-sharing-dataset/hour-fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
       "         1.6000e+01],\n",
       "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
       "         4.0000e+01],\n",
       "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
       "         3.2000e+01],\n",
       "        ...,\n",
       "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
       "         9.0000e+01],\n",
       "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
       "         6.1000e+01],\n",
       "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
       "         4.9000e+01]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we conver the date string to numbers corresponding to day of the month in column 1\n",
    "bikes_numpy = np.loadtxt(\"./data/p1ch4/bike-sharing-dataset/hour-fixed.csv\",\n",
    "                         dtype=np.float32,\n",
    "                         delimiter=\",\",\n",
    "                         skiprows=1,\n",
    "                         converters={1: lambda x: float(x[8:10])})\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here rows are successive time points in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17520, 17]), (17, 1))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#17520 hiurs, 17 columns \n",
    "#we will reshape to have 3 axis: day, hour, and 17 feature columns\n",
    "#stride tells us hor to how to navigate the flattened stored data\n",
    "#to move one hour move 17 places in storage \n",
    "bikes.shape, bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 24, 17]), (408, 17, 1))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the view method changes the way the tensor looks at the given data in storage \n",
    "#calling view returns a new tensor that changes the number of dimensions and striding info without changing storage \n",
    "#Allows you to rearrange data at no cost\n",
    "#-1 is a place hoder for \"left over indecies\" after the other dimensiots are assigned\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 17, 24]), (408, 1, 17))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want N x C x L ordering where N is num samples C is num of sequences L is length\n",
    "daily_bikes = daily_bikes.transpose(1,2)\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode Weather using Onehot\n",
    "first_day = bikes[:24].long()\n",
    "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
    "first_day[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot.scatter_(\n",
    "    dim=1,\n",
    "    index=first_day[:,9].unsqueeze(1).long() -1,\n",
    "    value = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n",
       "          0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n",
       "         16.0000,  1.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate to orignal data set\n",
    "torch.cat((bikes[:24], weather_onehot), 1)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now create daily weather onehot\n",
    "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_onehot.scatter_(1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat along C \n",
    "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) / 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "temp = daily_bikes[:, 10, :]\n",
    "temp_min = torch.min(temp)\n",
    "temp_max = torch.max(temp)\n",
    "daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - temp_min) / (temp_max - temp_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[14]:\n",
    "temp = daily_bikes[:, 10, :]\n",
    "daily_bikes[:, 10, :] = (daily_bikes[:, 10, :] - torch.mean(temp)) / torch.std(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Text Data </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Text Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text analysis is NLP natural language processing\n",
    "#we will start off with character based convert 2 tensor , then word based\n",
    "#Load Pride and Prejudice \n",
    "\n",
    "\n",
    "with open('./data/p1ch4/jane-austen/1342-0.txt', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can OneHotEncode on a character level\n",
    "#we will make all characters lowecrcase and ignore punctuation and numbers\n",
    "#each character will be represented as a vector\n",
    "\n",
    "lines = text.split('\\n')\n",
    "line = lines[200]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 128])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tensor for line\n",
    "letter_t = torch.zeros(len(line),128)\n",
    "letter_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we set 1 in the right position to represent a given character on each row\n",
    "# In[5]:\n",
    "for i, letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_t[i][letter_index] = 1\n",
    "#This sentence has now been onehotencoded\n",
    "#We can also use word or character embeddings to do thi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define clean_words \n",
    "def clean_words(input_str):\n",
    "    '''\n",
    "    Input:\n",
    "        input_str: input string text to be cleaned. Text will\n",
    "        be made lowercase, and punctuation will be removed.\n",
    "    Output:\n",
    "        word_list: list of lowercase words from input_str\n",
    "    \n",
    "    '''\n",
    "    punctuation = '.,;:\"!?_-`\"'\n",
    "    word_list = input_str.lower().replace('\\n', ' ').split()\n",
    "    word_list = [word.strip(punctuation) for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him',\n",
       " ['“impossible',\n",
       "  'mr',\n",
       "  'bennet',\n",
       "  'impossible',\n",
       "  'when',\n",
       "  'i',\n",
       "  'am',\n",
       "  'not',\n",
       "  'acquainted',\n",
       "  'with',\n",
       "  'him'])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_line= clean_words(line)\n",
    "line, words_in_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8484, 3828)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next build a mapping of words to indecies in your encoding\n",
    "word_list= sorted(set(clean_words(text)))\n",
    "word2index_dict= {word: i for (i, word) in enumerate(word_list)}\n",
    "\n",
    "len(word2index_dict), word2index_dict['impossible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '#1342]': 1,\n",
       " '$5,000)': 2,\n",
       " \"'_she\": 3,\n",
       " \"'ah\": 4,\n",
       " \"'as-is'\": 5,\n",
       " \"'bingley\": 6,\n",
       " \"'had\": 7,\n",
       " \"'having\": 8,\n",
       " \"'i\": 9,\n",
       " \"'keep\": 10,\n",
       " \"'lady\": 11,\n",
       " \"'lately\": 12,\n",
       " \"'lydia\": 13,\n",
       " \"'mr\": 14,\n",
       " \"'oh\": 15,\n",
       " \"'s\": 16,\n",
       " \"'this\": 17,\n",
       " \"'tis\": 18,\n",
       " \"'violently\": 19,\n",
       " \"'yes,'\": 20,\n",
       " \"'you\": 21,\n",
       " '($1': 22,\n",
       " '(801)': 23,\n",
       " '(a)': 24,\n",
       " '(an': 25,\n",
       " '(and': 26,\n",
       " '(any': 27,\n",
       " '(available': 28,\n",
       " '(b)': 29,\n",
       " '(by': 30,\n",
       " '(c)': 31,\n",
       " '(comparatively': 32,\n",
       " '(does': 33,\n",
       " '(for': 34,\n",
       " '(glancing': 35,\n",
       " '(if': 36,\n",
       " '(lady': 37,\n",
       " '(like': 38,\n",
       " '(most': 39,\n",
       " '(my': 40,\n",
       " '(or': 41,\n",
       " '(trademark/copyright)': 42,\n",
       " '(unasked': 43,\n",
       " '(what': 44,\n",
       " '(who': 45,\n",
       " '(www.gutenberg.org)': 46,\n",
       " '(“the': 47,\n",
       " '*': 48,\n",
       " '***': 49,\n",
       " '*****': 50,\n",
       " '1': 51,\n",
       " '1.a': 52,\n",
       " '1.b': 53,\n",
       " '1.c': 54,\n",
       " '1.d': 55,\n",
       " '1.e': 56,\n",
       " '1.e.1': 57,\n",
       " '1.e.2': 58,\n",
       " '1.e.3': 59,\n",
       " '1.e.4': 60,\n",
       " '1.e.5': 61,\n",
       " '1.e.6': 62,\n",
       " '1.e.7': 63,\n",
       " '1.e.8': 64,\n",
       " '1.e.9': 65,\n",
       " '1.f': 66,\n",
       " '1.f.1': 67,\n",
       " '1.f.2': 68,\n",
       " '1.f.3': 69,\n",
       " '1.f.4': 70,\n",
       " '1.f.5': 71,\n",
       " '1.f.6': 72,\n",
       " '10': 73,\n",
       " '11': 74,\n",
       " '12': 75,\n",
       " '13': 76,\n",
       " '1342-0.txt': 77,\n",
       " '1342-0.zip': 78,\n",
       " '14': 79,\n",
       " '15': 80,\n",
       " '1500': 81,\n",
       " '15th': 82,\n",
       " '16': 83,\n",
       " '17': 84,\n",
       " '18': 85,\n",
       " '18th': 86,\n",
       " '19': 87,\n",
       " '1998': 88,\n",
       " '2': 89,\n",
       " '20': 90,\n",
       " '20%': 91,\n",
       " '2001': 92,\n",
       " '2008': 93,\n",
       " '2018': 94,\n",
       " '21': 95,\n",
       " '22': 96,\n",
       " '23': 97,\n",
       " '24': 98,\n",
       " '25': 99,\n",
       " '26': 100,\n",
       " '26th': 101,\n",
       " '27': 102,\n",
       " '28': 103,\n",
       " '29': 104,\n",
       " '3': 105,\n",
       " '30': 106,\n",
       " '31': 107,\n",
       " '32': 108,\n",
       " '33': 109,\n",
       " '34': 110,\n",
       " '35': 111,\n",
       " '36': 112,\n",
       " '37': 113,\n",
       " '38': 114,\n",
       " '39': 115,\n",
       " '4': 116,\n",
       " '40': 117,\n",
       " '41': 118,\n",
       " '42': 119,\n",
       " '43': 120,\n",
       " '44': 121,\n",
       " '45': 122,\n",
       " '4557': 123,\n",
       " '46': 124,\n",
       " '47': 125,\n",
       " '48': 126,\n",
       " '49': 127,\n",
       " '5': 128,\n",
       " '50': 129,\n",
       " '501(c)(3)': 130,\n",
       " '51': 131,\n",
       " '52': 132,\n",
       " '53': 133,\n",
       " '54': 134,\n",
       " '55': 135,\n",
       " '56': 136,\n",
       " '57': 137,\n",
       " '58': 138,\n",
       " '59': 139,\n",
       " '596-1887': 140,\n",
       " '6': 141,\n",
       " '60': 142,\n",
       " '61': 143,\n",
       " '64-6221541': 144,\n",
       " '7': 145,\n",
       " '8': 146,\n",
       " '809': 147,\n",
       " '84116': 148,\n",
       " '9': 149,\n",
       " '90': 150,\n",
       " '99712': 151,\n",
       " '[ebook': 152,\n",
       " 'a': 153,\n",
       " 'a-shooting': 154,\n",
       " 'abatement': 155,\n",
       " 'abhorrence': 156,\n",
       " 'abhorrence.”': 157,\n",
       " 'abhorrent': 158,\n",
       " 'abide': 159,\n",
       " 'abiding': 160,\n",
       " 'abilities': 161,\n",
       " 'able': 162,\n",
       " 'able,”': 163,\n",
       " 'ablution': 164,\n",
       " 'abode': 165,\n",
       " 'abominable': 166,\n",
       " 'abominably': 167,\n",
       " 'abominate': 168,\n",
       " 'abound': 169,\n",
       " 'about': 170,\n",
       " 'about.”': 171,\n",
       " 'above': 172,\n",
       " 'abroad': 173,\n",
       " 'abroad?”': 174,\n",
       " 'abrupt': 175,\n",
       " 'abruptly': 176,\n",
       " 'abruptness': 177,\n",
       " 'absence': 178,\n",
       " 'absent': 179,\n",
       " 'absolute': 180,\n",
       " 'absolutely': 181,\n",
       " 'absurd': 182,\n",
       " 'absurdities': 183,\n",
       " 'absurdity': 184,\n",
       " 'abundant': 185,\n",
       " 'abundantly': 186,\n",
       " 'abuse': 187,\n",
       " 'abused': 188,\n",
       " 'abusing': 189,\n",
       " 'abusive': 190,\n",
       " 'accede': 191,\n",
       " 'acceded': 192,\n",
       " 'acceding': 193,\n",
       " 'accent': 194,\n",
       " 'accents': 195,\n",
       " 'accept': 196,\n",
       " 'acceptable': 197,\n",
       " 'acceptable.”': 198,\n",
       " 'acceptance': 199,\n",
       " 'accepted': 200,\n",
       " 'accepting': 201,\n",
       " 'access': 202,\n",
       " 'accessed': 203,\n",
       " 'accessible': 204,\n",
       " 'accident': 205,\n",
       " 'accidental': 206,\n",
       " 'accidentally': 207,\n",
       " 'accompanied': 208,\n",
       " 'accompany': 209,\n",
       " 'accompanying': 210,\n",
       " 'accomplished': 211,\n",
       " 'accomplished!--she': 212,\n",
       " 'accomplished.”': 213,\n",
       " 'accomplishment': 214,\n",
       " 'accomplishments': 215,\n",
       " 'accomplishments,”': 216,\n",
       " 'accordance': 217,\n",
       " 'according': 218,\n",
       " 'accordingly': 219,\n",
       " 'accordingly.”': 220,\n",
       " 'accosted': 221,\n",
       " 'account': 222,\n",
       " 'accounted': 223,\n",
       " 'accounting': 224,\n",
       " 'accounts': 225,\n",
       " 'accuracy': 226,\n",
       " 'accurate': 227,\n",
       " 'accusation': 228,\n",
       " 'accusations': 229,\n",
       " 'accuse': 230,\n",
       " 'accused': 231,\n",
       " 'accusing': 232,\n",
       " 'accustomed': 233,\n",
       " 'ached': 234,\n",
       " 'achieving': 235,\n",
       " 'acknowledge': 236,\n",
       " 'acknowledge?”': 237,\n",
       " 'acknowledged': 238,\n",
       " 'acknowledging': 239,\n",
       " 'acknowledgment': 240,\n",
       " 'acknowledgments': 241,\n",
       " 'acquaint': 242,\n",
       " 'acquaintance': 243,\n",
       " 'acquaintance--an': 244,\n",
       " 'acquaintance.”': 245,\n",
       " 'acquaintances': 246,\n",
       " 'acquainted': 247,\n",
       " 'acquainted--”': 248,\n",
       " 'acquainting': 249,\n",
       " 'acquiesce': 250,\n",
       " 'acquiescence': 251,\n",
       " 'acquire': 252,\n",
       " 'acquired': 253,\n",
       " 'acquisition': 254,\n",
       " 'acquit': 255,\n",
       " 'acrimony': 256,\n",
       " 'across': 257,\n",
       " 'act': 258,\n",
       " 'acted': 259,\n",
       " 'acted!”': 260,\n",
       " 'acting': 261,\n",
       " 'action': 262,\n",
       " 'actions': 263,\n",
       " 'actions--may': 264,\n",
       " 'active': 265,\n",
       " 'activity': 266,\n",
       " 'actual': 267,\n",
       " 'actually': 268,\n",
       " 'actuated': 269,\n",
       " 'acute': 270,\n",
       " 'acutely': 271,\n",
       " 'acutest': 272,\n",
       " 'adapted': 273,\n",
       " 'add': 274,\n",
       " 'added': 275,\n",
       " 'adding': 276,\n",
       " 'addition': 277,\n",
       " 'additional': 278,\n",
       " 'additions': 279,\n",
       " 'address': 280,\n",
       " 'addressed': 281,\n",
       " 'addresses': 282,\n",
       " 'addresses.”': 283,\n",
       " 'addressing': 284,\n",
       " 'adds': 285,\n",
       " 'adept': 286,\n",
       " 'adequate': 287,\n",
       " 'adhered': 288,\n",
       " 'adhering': 289,\n",
       " 'adieu': 290,\n",
       " 'adieus': 291,\n",
       " 'adjusting': 292,\n",
       " 'admirable': 293,\n",
       " 'admirable!”': 294,\n",
       " 'admiration': 295,\n",
       " 'admire': 296,\n",
       " 'admired': 297,\n",
       " 'admirer': 298,\n",
       " 'admires': 299,\n",
       " 'admiring': 300,\n",
       " 'admission': 301,\n",
       " 'admit': 302,\n",
       " 'admittance': 303,\n",
       " 'admitted': 304,\n",
       " 'admitting': 305,\n",
       " 'adopt': 306,\n",
       " 'adopted': 307,\n",
       " 'adoration': 308,\n",
       " 'adorned': 309,\n",
       " 'advance': 310,\n",
       " 'advanced': 311,\n",
       " 'advancement': 312,\n",
       " 'advances': 313,\n",
       " 'advantage': 314,\n",
       " 'advantage.”': 315,\n",
       " 'advantageous': 316,\n",
       " 'advantageously': 317,\n",
       " 'advantages': 318,\n",
       " 'adventure': 319,\n",
       " 'advertised': 320,\n",
       " 'advice': 321,\n",
       " 'advisable': 322,\n",
       " 'advise': 323,\n",
       " 'advised': 324,\n",
       " 'advising': 325,\n",
       " 'affability': 326,\n",
       " 'affable': 327,\n",
       " 'affair': 328,\n",
       " 'affair,”': 329,\n",
       " 'affair.”': 330,\n",
       " 'affairs': 331,\n",
       " 'affect': 332,\n",
       " 'affectation': 333,\n",
       " 'affected': 334,\n",
       " 'affection': 335,\n",
       " 'affection,”': 336,\n",
       " 'affection.”': 337,\n",
       " 'affectionate': 338,\n",
       " 'affectionately': 339,\n",
       " 'affections': 340,\n",
       " 'affinity': 341,\n",
       " 'affirmative': 342,\n",
       " 'afflicted': 343,\n",
       " 'afflicting': 344,\n",
       " 'affliction': 345,\n",
       " 'afford': 346,\n",
       " 'afforded': 347,\n",
       " 'affording': 348,\n",
       " 'affords': 349,\n",
       " 'affront': 350,\n",
       " 'affronted': 351,\n",
       " 'afraid': 352,\n",
       " 'afresh': 353,\n",
       " 'after': 354,\n",
       " 'after--the': 355,\n",
       " 'afternoon': 356,\n",
       " 'afterwards': 357,\n",
       " 'again': 358,\n",
       " 'again!”': 359,\n",
       " 'again.”': 360,\n",
       " 'again?”': 361,\n",
       " 'against': 362,\n",
       " 'age': 363,\n",
       " 'age.”': 364,\n",
       " 'age?”': 365,\n",
       " 'agent': 366,\n",
       " 'agitated': 367,\n",
       " 'agitation': 368,\n",
       " 'agitations': 369,\n",
       " 'ago': 370,\n",
       " 'ago.”': 371,\n",
       " 'agonies': 372,\n",
       " 'agony': 373,\n",
       " 'agree': 374,\n",
       " 'agreeable': 375,\n",
       " 'agreeable--allowing': 376,\n",
       " 'agreeable-looking': 377,\n",
       " 'agreeable.”': 378,\n",
       " 'agreeably': 379,\n",
       " 'agreed': 380,\n",
       " 'agreeing': 381,\n",
       " 'agreement': 382,\n",
       " 'aid': 383,\n",
       " 'aimed': 384,\n",
       " 'air': 385,\n",
       " 'airing': 386,\n",
       " 'airs': 387,\n",
       " 'ak': 388,\n",
       " 'alacrity': 389,\n",
       " 'alarm': 390,\n",
       " 'alarm,”': 391,\n",
       " 'alarmed': 392,\n",
       " 'alarming': 393,\n",
       " 'alarms': 394,\n",
       " 'alas': 395,\n",
       " 'alienated': 396,\n",
       " 'alighted': 397,\n",
       " 'alike': 398,\n",
       " 'alive': 399,\n",
       " 'all': 400,\n",
       " 'all,”': 401,\n",
       " 'all--all': 402,\n",
       " 'all--and': 403,\n",
       " 'all.”': 404,\n",
       " 'all?”': 405,\n",
       " 'allayed': 406,\n",
       " 'alleviate': 407,\n",
       " 'alleviated': 408,\n",
       " 'alliance': 409,\n",
       " 'allow': 410,\n",
       " 'allow--and': 411,\n",
       " 'allowable': 412,\n",
       " 'allowance': 413,\n",
       " 'allowances': 414,\n",
       " 'allowed': 415,\n",
       " 'allowing': 416,\n",
       " 'allude': 417,\n",
       " 'alluded': 418,\n",
       " 'alluding': 419,\n",
       " 'allurements': 420,\n",
       " 'allusion': 421,\n",
       " 'allusions': 422,\n",
       " 'almost': 423,\n",
       " 'alone': 424,\n",
       " 'alone.”': 425,\n",
       " 'along': 426,\n",
       " 'aloof': 427,\n",
       " 'aloud': 428,\n",
       " 'aloud,”': 429,\n",
       " 'already': 430,\n",
       " 'also': 431,\n",
       " 'altar': 432,\n",
       " 'alter': 433,\n",
       " 'alteration': 434,\n",
       " 'alterations': 435,\n",
       " 'altered': 436,\n",
       " 'altered--what': 437,\n",
       " 'alternate': 438,\n",
       " 'alternative': 439,\n",
       " 'although': 440,\n",
       " 'altogether': 441,\n",
       " 'altogether--mr': 442,\n",
       " 'always': 443,\n",
       " 'always,”': 444,\n",
       " 'am': 445,\n",
       " 'am!”': 446,\n",
       " 'am,”': 447,\n",
       " 'am--and': 448,\n",
       " 'am?”': 449,\n",
       " 'amaze': 450,\n",
       " 'amazed': 451,\n",
       " 'amazement': 452,\n",
       " 'amazes': 453,\n",
       " 'amazing': 454,\n",
       " 'amazing!--but': 455,\n",
       " 'ambition': 456,\n",
       " 'amendment': 457,\n",
       " 'amends': 458,\n",
       " 'amends--but': 459,\n",
       " 'amends--of': 460,\n",
       " 'amiable': 461,\n",
       " 'amiable)': 462,\n",
       " 'amiable”--but': 463,\n",
       " 'amid': 464,\n",
       " 'amidst': 465,\n",
       " 'amiss': 466,\n",
       " 'among': 467,\n",
       " 'amongst': 468,\n",
       " 'amount': 469,\n",
       " 'amounting': 470,\n",
       " 'ample': 471,\n",
       " 'amply': 472,\n",
       " 'amuse': 473,\n",
       " 'amused': 474,\n",
       " 'amusement': 475,\n",
       " 'amusement,”': 476,\n",
       " 'amusements': 477,\n",
       " 'amusing': 478,\n",
       " 'an': 479,\n",
       " 'ancient--though': 480,\n",
       " 'and': 481,\n",
       " 'anecdote': 482,\n",
       " 'anecdotes': 483,\n",
       " 'anew': 484,\n",
       " 'angel': 485,\n",
       " 'angelic': 486,\n",
       " 'anger': 487,\n",
       " 'angrily': 488,\n",
       " 'angry': 489,\n",
       " 'angry.”': 490,\n",
       " 'anguish': 491,\n",
       " 'animal': 492,\n",
       " 'animated': 493,\n",
       " 'animating': 494,\n",
       " 'animation': 495,\n",
       " 'ankle': 496,\n",
       " 'ankles': 497,\n",
       " 'anne': 498,\n",
       " \"anne's\": 499,\n",
       " 'annesley': 500,\n",
       " 'annexed': 501,\n",
       " 'announce': 502,\n",
       " 'announced': 503,\n",
       " 'annoyed': 504,\n",
       " 'annual': 505,\n",
       " 'annum': 506,\n",
       " 'anonymous': 507,\n",
       " 'another': 508,\n",
       " 'answer': 509,\n",
       " 'answer--but': 510,\n",
       " 'answer.”': 511,\n",
       " 'answerable': 512,\n",
       " 'answered': 513,\n",
       " 'answered.”': 514,\n",
       " 'answering': 515,\n",
       " 'answers': 516,\n",
       " 'antagonist': 517,\n",
       " 'ante-chamber': 518,\n",
       " 'anticipate': 519,\n",
       " 'anticipated': 520,\n",
       " 'anticipating': 521,\n",
       " 'anticipation': 522,\n",
       " 'anxiety': 523,\n",
       " 'anxious': 524,\n",
       " 'anxiously': 525,\n",
       " 'any': 526,\n",
       " 'any)': 527,\n",
       " 'any_.”': 528,\n",
       " 'anybody': 529,\n",
       " \"anybody's\": 530,\n",
       " 'anyhow': 531,\n",
       " 'anyone': 532,\n",
       " 'anyone.”': 533,\n",
       " 'anything': 534,\n",
       " 'anything!”': 535,\n",
       " 'anything.”': 536,\n",
       " 'anywhere': 537,\n",
       " 'apace': 538,\n",
       " 'apartment': 539,\n",
       " 'apartments': 540,\n",
       " 'apologies': 541,\n",
       " 'apologise': 542,\n",
       " 'apologise.”': 543,\n",
       " 'apologised': 544,\n",
       " 'apologising': 545,\n",
       " 'apology': 546,\n",
       " 'apology.”': 547,\n",
       " 'apothecary': 548,\n",
       " 'apparel': 549,\n",
       " 'apparent': 550,\n",
       " 'apparently': 551,\n",
       " 'appeal': 552,\n",
       " 'appeals': 553,\n",
       " 'appear': 554,\n",
       " 'appearance': 555,\n",
       " 'appearance,”': 556,\n",
       " 'appearances': 557,\n",
       " 'appeared': 558,\n",
       " 'appearing': 559,\n",
       " 'appears': 560,\n",
       " 'appease': 561,\n",
       " 'appeased': 562,\n",
       " 'appertain': 563,\n",
       " 'appetite': 564,\n",
       " 'applicable': 565,\n",
       " 'application': 566,\n",
       " 'applied': 567,\n",
       " 'applies': 568,\n",
       " 'apply': 569,\n",
       " 'applying': 570,\n",
       " 'appointed': 571,\n",
       " 'appointment': 572,\n",
       " 'apprehended': 573,\n",
       " 'apprehending': 574,\n",
       " 'apprehension': 575,\n",
       " 'apprehensions': 576,\n",
       " 'apprehensive': 577,\n",
       " 'approach': 578,\n",
       " 'approached': 579,\n",
       " 'approaching': 580,\n",
       " 'approbation': 581,\n",
       " 'approve': 582,\n",
       " 'approved': 583,\n",
       " 'april': 584,\n",
       " 'apt': 585,\n",
       " 'arch': 586,\n",
       " 'archbishop': 587,\n",
       " 'archive': 588,\n",
       " 'archly': 589,\n",
       " 'archness': 590,\n",
       " 'ardent': 591,\n",
       " 'ardently': 592,\n",
       " 'are': 593,\n",
       " 'are)': 594,\n",
       " 'are,”': 595,\n",
       " 'are.”': 596,\n",
       " 'argued': 597,\n",
       " 'arguing': 598,\n",
       " 'argument': 599,\n",
       " 'arguments': 600,\n",
       " 'arise': 601,\n",
       " 'arisen': 602,\n",
       " 'arisen?”': 603,\n",
       " 'arising': 604,\n",
       " 'arm': 605,\n",
       " 'arm-in-arm': 606,\n",
       " 'army': 607,\n",
       " 'army,”': 608,\n",
       " 'arose': 609,\n",
       " 'around': 610,\n",
       " 'aroused': 611,\n",
       " 'arrange': 612,\n",
       " 'arranged': 613,\n",
       " 'arrangement': 614,\n",
       " 'arrangements': 615,\n",
       " 'arranges': 616,\n",
       " 'arranging': 617,\n",
       " 'array': 618,\n",
       " 'arrear': 619,\n",
       " 'arrested': 620,\n",
       " 'arrival': 621,\n",
       " 'arrival!”': 622,\n",
       " 'arrive': 623,\n",
       " 'arrived': 624,\n",
       " 'arrived--that': 625,\n",
       " 'arriving': 626,\n",
       " 'arrogance': 627,\n",
       " 'arrogant': 628,\n",
       " 'art': 629,\n",
       " 'art.”': 630,\n",
       " 'artful': 631,\n",
       " 'artfully': 632,\n",
       " 'article': 633,\n",
       " 'articles': 634,\n",
       " 'artificial': 635,\n",
       " 'arts': 636,\n",
       " 'arts,”': 637,\n",
       " 'as': 638,\n",
       " 'ascended': 639,\n",
       " 'ascertain': 640,\n",
       " 'ascertaining': 641,\n",
       " 'ascii”': 642,\n",
       " 'ashamed': 643,\n",
       " 'ashworth': 644,\n",
       " 'aside': 645,\n",
       " 'ask': 646,\n",
       " 'ask,”': 647,\n",
       " 'ask--”': 648,\n",
       " 'ask.”': 649,\n",
       " 'ask?--”': 650,\n",
       " 'asked': 651,\n",
       " 'asking': 652,\n",
       " 'aspect': 653,\n",
       " 'asperity': 654,\n",
       " 'aspire': 655,\n",
       " 'assembled': 656,\n",
       " 'assemblies': 657,\n",
       " 'assembly': 658,\n",
       " 'assent': 659,\n",
       " 'assented': 660,\n",
       " 'assents': 661,\n",
       " 'assert': 662,\n",
       " 'asserted': 663,\n",
       " 'asserting': 664,\n",
       " 'assertion': 665,\n",
       " 'assertions': 666,\n",
       " 'assiduous': 667,\n",
       " 'assiduously': 668,\n",
       " 'assist': 669,\n",
       " 'assistance': 670,\n",
       " 'assistant': 671,\n",
       " 'assisted': 672,\n",
       " 'assisting': 673,\n",
       " 'associated': 674,\n",
       " 'associated)': 675,\n",
       " 'associating': 676,\n",
       " 'assume': 677,\n",
       " 'assumed': 678,\n",
       " 'assurance': 679,\n",
       " 'assurances': 680,\n",
       " 'assure': 681,\n",
       " 'assured': 682,\n",
       " 'assuring': 683,\n",
       " 'astonish': 684,\n",
       " 'astonish--and': 685,\n",
       " 'astonished': 686,\n",
       " 'astonished,”': 687,\n",
       " 'astonishment': 688,\n",
       " 'at': 689,\n",
       " 'at!”': 690,\n",
       " 'ate': 691,\n",
       " 'atone': 692,\n",
       " 'atoned': 693,\n",
       " 'atonement': 694,\n",
       " 'atonement--for': 695,\n",
       " 'attach': 696,\n",
       " 'attached': 697,\n",
       " 'attachment': 698,\n",
       " 'attachments': 699,\n",
       " 'attack': 700,\n",
       " 'attacked': 701,\n",
       " 'attacks': 702,\n",
       " 'attained': 703,\n",
       " 'attempt': 704,\n",
       " 'attempt.”': 705,\n",
       " 'attempted': 706,\n",
       " 'attempting': 707,\n",
       " 'attend': 708,\n",
       " 'attendance': 709,\n",
       " 'attendant': 710,\n",
       " 'attendants': 711,\n",
       " 'attended': 712,\n",
       " 'attending': 713,\n",
       " 'attention': 714,\n",
       " 'attentions': 715,\n",
       " 'attentive': 716,\n",
       " 'attentively': 717,\n",
       " 'attics': 718,\n",
       " 'attitude.”': 719,\n",
       " 'attorney': 720,\n",
       " 'attracted': 721,\n",
       " 'attraction': 722,\n",
       " 'attractions': 723,\n",
       " 'attribute': 724,\n",
       " 'attributed': 725,\n",
       " 'attributing': 726,\n",
       " 'audible': 727,\n",
       " 'audience': 728,\n",
       " 'aught': 729,\n",
       " 'augment': 730,\n",
       " 'augmented': 731,\n",
       " 'august': 732,\n",
       " 'aunt': 733,\n",
       " 'aunt!”': 734,\n",
       " \"aunt's\": 735,\n",
       " 'aunt,”': 736,\n",
       " 'aunts': 737,\n",
       " 'austen': 738,\n",
       " 'austerity': 739,\n",
       " 'author': 740,\n",
       " 'authorise': 741,\n",
       " 'authorised': 742,\n",
       " 'authorising': 743,\n",
       " 'authoritative': 744,\n",
       " 'authority': 745,\n",
       " 'authorized': 746,\n",
       " 'avail': 747,\n",
       " 'available': 748,\n",
       " 'availed': 749,\n",
       " 'avarice': 750,\n",
       " 'avenue.”': 751,\n",
       " 'avoid': 752,\n",
       " 'avoidance': 753,\n",
       " 'avoided': 754,\n",
       " 'avoiding': 755,\n",
       " 'avowal': 756,\n",
       " 'avowed': 757,\n",
       " 'awaited': 758,\n",
       " 'awake': 759,\n",
       " 'awakened': 760,\n",
       " 'aware': 761,\n",
       " 'aware,”': 762,\n",
       " 'away': 763,\n",
       " 'away,”': 764,\n",
       " 'away--you': 765,\n",
       " 'away.”': 766,\n",
       " 'away?”': 767,\n",
       " 'awe': 768,\n",
       " 'awed': 769,\n",
       " 'awful': 770,\n",
       " 'awkward': 771,\n",
       " 'awkwardness': 772,\n",
       " 'awoke': 773,\n",
       " 'b': 774,\n",
       " 'back': 775,\n",
       " 'backed': 776,\n",
       " 'backgammon': 777,\n",
       " 'backward': 778,\n",
       " 'backwards': 779,\n",
       " 'bad': 780,\n",
       " 'bad--belongs': 781,\n",
       " 'bad?”': 782,\n",
       " 'bakewell': 783,\n",
       " 'ball': 784,\n",
       " 'ball,”': 785,\n",
       " 'ball--and': 786,\n",
       " 'ball-room': 787,\n",
       " 'ball.”': 788,\n",
       " 'balls': 789,\n",
       " 'balm': 790,\n",
       " 'bandbox': 791,\n",
       " 'banish': 792,\n",
       " 'banished': 793,\n",
       " 'banks': 794,\n",
       " 'barbarous': 795,\n",
       " 'barbarously': 796,\n",
       " 'bare': 797,\n",
       " 'barefaced': 798,\n",
       " 'barely': 799,\n",
       " 'barnet': 800,\n",
       " 'barouche-box': 801,\n",
       " 'based': 802,\n",
       " 'bashful': 803,\n",
       " 'basis': 804,\n",
       " 'bath': 805,\n",
       " 'bathing-place': 806,\n",
       " 'battled': 807,\n",
       " 'be': 808,\n",
       " 'be!--engaged': 809,\n",
       " 'be,”': 810,\n",
       " 'be--but': 811,\n",
       " 'be.”': 812,\n",
       " 'be?”': 813,\n",
       " 'bear': 814,\n",
       " 'bearing': 815,\n",
       " 'bears': 816,\n",
       " 'beatings': 817,\n",
       " 'beauteous': 818,\n",
       " 'beauties': 819,\n",
       " 'beautiful': 820,\n",
       " 'beauty': 821,\n",
       " 'beauty!--i': 822,\n",
       " 'beauty.”': 823,\n",
       " 'beaux': 824,\n",
       " 'became': 825,\n",
       " 'because': 826,\n",
       " 'become': 827,\n",
       " 'becomes': 828,\n",
       " 'becoming': 829,\n",
       " 'bed': 830,\n",
       " 'bedrooms': 831,\n",
       " 'been': 832,\n",
       " 'been,”': 833,\n",
       " 'befall': 834,\n",
       " 'before': 835,\n",
       " 'before)': 836,\n",
       " 'before,”': 837,\n",
       " 'before.”': 838,\n",
       " 'beforehand': 839,\n",
       " 'befriended': 840,\n",
       " 'beg': 841,\n",
       " 'began': 842,\n",
       " 'begged': 843,\n",
       " 'begging': 844,\n",
       " 'begin': 845,\n",
       " 'begin?”': 846,\n",
       " 'beginning': 847,\n",
       " 'beginning--from': 848,\n",
       " 'begins': 849,\n",
       " 'begins--but': 850,\n",
       " 'begs': 851,\n",
       " 'begun': 852,\n",
       " 'begun.”': 853,\n",
       " 'behalf': 854,\n",
       " 'behave': 855,\n",
       " 'behaved': 856,\n",
       " 'behaved?”': 857,\n",
       " 'behaves': 858,\n",
       " 'behaves,”': 859,\n",
       " 'behaviour': 860,\n",
       " 'beheld': 861,\n",
       " 'behind': 862,\n",
       " 'beholding': 863,\n",
       " 'being': 864,\n",
       " 'belief': 865,\n",
       " 'believe': 866,\n",
       " 'believe,”': 867,\n",
       " 'believed': 868,\n",
       " 'believed!”': 869,\n",
       " 'believes': 870,\n",
       " 'believing': 871,\n",
       " 'bell': 872,\n",
       " 'bell--i': 873,\n",
       " 'belong': 874,\n",
       " 'belonged': 875,\n",
       " 'belonging': 876,\n",
       " 'belongs': 877,\n",
       " 'beloved': 878,\n",
       " 'below': 879,\n",
       " 'benches': 880,\n",
       " 'beneath': 881,\n",
       " 'beneficence': 882,\n",
       " 'beneficial': 883,\n",
       " 'benefit': 884,\n",
       " 'benefited': 885,\n",
       " 'benefiting': 886,\n",
       " 'benefits': 887,\n",
       " 'benevolence': 888,\n",
       " 'benevolence,”': 889,\n",
       " 'benevolent': 890,\n",
       " 'bennet': 891,\n",
       " 'bennet!”': 892,\n",
       " \"bennet's\": 893,\n",
       " \"bennet,'\": 894,\n",
       " 'bennet,”': 895,\n",
       " \"bennet.'--my\": 896,\n",
       " 'bennet.”': 897,\n",
       " 'bennet?”': 898,\n",
       " 'bennets': 899,\n",
       " 'bent': 900,\n",
       " 'bequeathed': 901,\n",
       " 'bequest': 902,\n",
       " 'besides': 903,\n",
       " 'best': 904,\n",
       " 'best.”': 905,\n",
       " 'bestow': 906,\n",
       " 'bestow!--how': 907,\n",
       " 'bestow.”': 908,\n",
       " 'bestowed': 909,\n",
       " 'bestowing': 910,\n",
       " 'betray': 911,\n",
       " 'betrayed': 912,\n",
       " 'bets': 913,\n",
       " 'better': 914,\n",
       " 'better,”': 915,\n",
       " 'better.”': 916,\n",
       " 'between': 917,\n",
       " 'bewailed': 918,\n",
       " 'bewildered': 919,\n",
       " 'bewitched': 920,\n",
       " 'bewitching': 921,\n",
       " 'beyond': 922,\n",
       " 'bid': 923,\n",
       " 'bidding': 924,\n",
       " 'billiard-table': 925,\n",
       " 'bills': 926,\n",
       " 'binary': 927,\n",
       " 'bingley': 928,\n",
       " \"bingley's\": 929,\n",
       " 'bingley)': 930,\n",
       " 'bingley,”': 931,\n",
       " 'bingley.”': 932,\n",
       " 'bingley?”': 933,\n",
       " 'bingleys': 934,\n",
       " \"bingleys'\": 935,\n",
       " 'bingley”': 936,\n",
       " 'birds': 937,\n",
       " 'birmingham': 938,\n",
       " 'birth': 939,\n",
       " 'bit': 940,\n",
       " 'biting': 941,\n",
       " 'bitter': 942,\n",
       " 'bitterest': 943,\n",
       " 'bitterly': 944,\n",
       " 'bitterness': 945,\n",
       " 'black': 946,\n",
       " 'blacken': 947,\n",
       " 'blame': 948,\n",
       " 'blameable': 949,\n",
       " 'blamed': 950,\n",
       " 'blameless': 951,\n",
       " 'blamelessness': 952,\n",
       " 'blaming': 953,\n",
       " 'blasted': 954,\n",
       " 'blemish': 955,\n",
       " 'blenheim': 956,\n",
       " 'bless': 957,\n",
       " 'blessed': 958,\n",
       " 'blessing': 959,\n",
       " 'blind': 960,\n",
       " 'blinded': 961,\n",
       " 'blinds': 962,\n",
       " 'blots': 963,\n",
       " 'blow': 964,\n",
       " 'blown': 965,\n",
       " 'blowsy!”': 966,\n",
       " 'blue': 967,\n",
       " 'blush': 968,\n",
       " 'blushed': 969,\n",
       " 'blushing': 970,\n",
       " 'board': 971,\n",
       " 'boast': 972,\n",
       " 'boast.”': 973,\n",
       " 'boasted': 974,\n",
       " 'boasting': 975,\n",
       " 'bodies': 976,\n",
       " 'body': 977,\n",
       " 'boisterously': 978,\n",
       " 'bold': 979,\n",
       " 'boldly': 980,\n",
       " 'bonnet': 981,\n",
       " 'book': 982,\n",
       " 'book-room': 983,\n",
       " 'book.”': 984,\n",
       " 'books': 985,\n",
       " 'books.”': 986,\n",
       " 'books?”': 987,\n",
       " 'bordered': 988,\n",
       " 'bordering': 989,\n",
       " 'bore': 990,\n",
       " 'born': 991,\n",
       " 'borne': 992,\n",
       " 'borrow': 993,\n",
       " 'borrowed': 994,\n",
       " 'bosom': 995,\n",
       " 'bosoms': 996,\n",
       " 'both': 997,\n",
       " 'both,”': 998,\n",
       " 'both--”': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is now the mapped word 2 index \n",
    "word2index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 8324 “impossible\n",
      " 1 4905 mr\n",
      " 2  891 bennet\n",
      " 3 3828 impossible\n",
      " 4 8017 when\n",
      " 5 3740 i\n",
      " 6  445 am\n",
      " 7 5054 not\n",
      " 8  247 acquainted\n",
      " 9 8094 with\n",
      "10 3619 him\n",
      "torch.Size([11, 8484])\n"
     ]
    }
   ],
   "source": [
    "word_t = torch.zeros(len(words_in_line), len(word2index_dict))\n",
    "for i, word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word]\n",
    "    word_t[i][word_index] = 1\n",
    "    print('{:2} {:4} {}'.format(i, word_index, word))\n",
    "\n",
    "print(word_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Text Embeddings </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3> Text Embeddings </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As text can be represented as binary digits it can also be represented \n",
    "#as floating point numbers\n",
    "#Embeddings are useful when onehotencodings become too comebrsome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
